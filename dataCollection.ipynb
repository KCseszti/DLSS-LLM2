{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Note:</h4>\n",
    "<p>This code section is the cleaned summary of the codes we used for the data collection.<br>\n",
    "Because the running time for Billboard was ~1h, and for Genius it was between 12-14hs, we didn't rerun the code and that's why there are no outputs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZD7bESTuoRs"
   },
   "outputs": [],
   "source": [
    "### Imports\n",
    "from unidecode import unidecode\n",
    "import lyricsgenius\n",
    "import os, re, time, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.rrule import rrule, WEEKLY\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7FsdZ48u2YW"
   },
   "source": [
    "<h3>Billboard Top100 data scraping from 2000 until 2025 (09.08).</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEd6qawjvAcQ"
   },
   "outputs": [],
   "source": [
    "NUM = re.compile(r\"\\d+\")\n",
    "def to_int(s):\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    if s == \"--\":\n",
    "        return None\n",
    "    m = NUM.search(s)\n",
    "    return int(m.group()) if m else None\n",
    "\n",
    "\n",
    "### Helper function for metric lables like - LW, Peak and Weeks\n",
    "def metric_by_label(row, label):\n",
    "    lab = row.find(\"span\", string=lambda t: isinstance(t, str) and label in t.upper())\n",
    "    if not lab:\n",
    "        return None\n",
    "\n",
    "    li = lab.find_next(lambda tag: tag.name == \"li\" and \"o-chart-results-list__item\" in (tag.get(\"class\") or []))\n",
    "    if li:\n",
    "        val_span = li.find(\"span\", class_=lambda c: c and \"c-label\" in c)\n",
    "        return to_int(val_span.get_text(strip=True)) if val_span else to_int(li.get_text(\" \", strip=True))\n",
    "\n",
    "    val_span = lab.find_next(lambda tag: tag.name == \"span\" and \"c-label\" in (tag.get(\"class\") or []))\n",
    "    return to_int(val_span.get_text(strip=True)) if val_span else None\n",
    "\n",
    "\n",
    "### Scrape data for one week\n",
    "def scrape_week(date_str=None):\n",
    "    url = f\"https://www.billboard.com/charts/hot-100/{date_str}/\" if date_str else \"https://www.billboard.com/charts/hot-100/\"\n",
    "    res = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=30)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    rows = []\n",
    "    for e in soup.find_all(attrs={'class': 'o-chart-results-list-row-container'}):\n",
    "        # title & artist\n",
    "        title_el = e.h3\n",
    "        if not title_el:\n",
    "            continue\n",
    "        title  = title_el.get_text(strip=True)\n",
    "        artist = title_el.find_next('span').get_text(strip=True)\n",
    "\n",
    "        # current rank\n",
    "        rank_el = e.select_one(\"span.c-label.a-font-primary-bold-l\") or e.find('span', class_='c-label')\n",
    "        rank = to_int(rank_el.get_text(strip=True)) if rank_el else None\n",
    "\n",
    "        # last week rank, peak and number of week in top100 (based on labels)\n",
    "        lw    = metric_by_label(e, \"LW\")\n",
    "        peak  = metric_by_label(e, \"PEAK\")\n",
    "        weeks = metric_by_label(e, \"WEEKS\")\n",
    "\n",
    "        if rank is not None:\n",
    "            rows.append({\n",
    "                \"chart_date\": date_str,\n",
    "                \"rank\": rank,\n",
    "                \"title\": title,\n",
    "                \"artist\": artist,\n",
    "                \"last_week\": lw,\n",
    "                \"peak\": peak,\n",
    "                \"weeks_on_chart\": weeks\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "\n",
    "### Scrape for multiple weeks - base\n",
    "def weekly_dates(start, end):\n",
    "    return [d.strftime(\"%Y-%m-%d\")\n",
    "            for d in rrule(WEEKLY,\n",
    "                           dtstart=datetime.fromisoformat(start),\n",
    "                           until=datetime.fromisoformat(end))]\n",
    "\n",
    "\n",
    "### Scrape for given range\n",
    "def scrape_range(start=\"\", end=\"\", pause=0.12):\n",
    "    all_rows = []\n",
    "    for ds in tqdm(weekly_dates(start, end), desc=\"Weeks\"):\n",
    "        try:\n",
    "            all_rows.extend(scrape_week(ds))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {ds}: {e}\")\n",
    "        time.sleep(pause)\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if not df.empty:\n",
    "        for col in [\"rank\", \"last_week\", \"peak\", \"weeks_on_chart\"]:\n",
    "            df[col] = df[col].map(to_int)\n",
    "        df.drop_duplicates(subset=[\"chart_date\",\"rank\",\"title\",\"artist\"], inplace=True)\n",
    "        df.sort_values([\"chart_date\",\"rank\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "### Run scraper\n",
    "df = scrape_range(\"2000-01-08\", \"2025-08-09\")\n",
    "print(\"Number of weeks:\", df[\"chart_date\"].nunique(), \" - Number of rows:\", len(df))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWxeCtgqvD_J"
   },
   "outputs": [],
   "source": [
    "### Setting column last_week to integer datatype\n",
    "df['last_week'] = pd.to_numeric(df['last_week'], errors='coerce').astype('Int64')\n",
    "df.to_json(\"hot100_2000_2025_raw.json\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wWTZvGUvH7P"
   },
   "outputs": [],
   "source": [
    "### Create new dataset with the unique artist-title pairs (for Genius API)\n",
    "unique_songs_df = df[[\"title\", \"artist\"]].drop_duplicates().reset_index(drop=True)\n",
    "unique_songs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU8-PnOpvLmO"
   },
   "source": [
    "<h3>Cleaning dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-0D0OnZvKnl"
   },
   "outputs": [],
   "source": [
    "### Normalize artist names\n",
    "def clean_artist_name(name):\n",
    "    # space around &\n",
    "    name = re.sub(r'\\s*&\\s*', ' & ', name)\n",
    "    # space around \"and\"\n",
    "    name = re.sub(r'\\s*(and)\\s*', r' and ', name, flags=re.IGNORECASE)\n",
    "    # space around \"Featuring\"\n",
    "    name = re.sub(r'\\s*(featuring)\\s*', r' Featuring ', name, flags=re.IGNORECASE)\n",
    "    # space around \"Feat\"\n",
    "    name = re.sub(r'\\s*(feat\\.)\\s*', r' Feat. ', name, flags=re.IGNORECASE)\n",
    "    # space around \"Presents\"\n",
    "    name = re.sub(r'\\s*(presents)\\s*', r' Presents ', name, flags=re.IGNORECASE)\n",
    "    # space around \"With\"\n",
    "    name = re.sub(r'\\s*(with)\\s*', r' With ', name, flags=re.IGNORECASE)\n",
    "    # space around \",\"\n",
    "    name = re.sub(r'\\s*,\\s*', ', ', name)\n",
    "    # remove extra spaces\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "\n",
    "    return name\n",
    "\n",
    "unique_songs_df[\"artist\"] = unique_songs_df[\"artist\"].apply(clean_artist_name)\n",
    "\n",
    "## later added adjustment based on the first results\n",
    "unique_songs_df[\"artist\"] = unique_songs_df[\"artist\"].str.replace(r\"Gr\\s*and\\s*e\", \"Grande\", regex=True)\n",
    "\n",
    "unique_songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc8UU8sPvRIT"
   },
   "outputs": [],
   "source": [
    "### Save dataset\n",
    "# unique_songs_df.to_json(\"hot100_2000_2025_cleaned.json\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksdXbAHNvc4Q"
   },
   "source": [
    "<h3>Genius API scraping</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9nhnyrRvf9_"
   },
   "outputs": [],
   "source": [
    "### Configuration\n",
    "TOKEN = \"Ro_egtJWqm49ss1KHS_N4VgC3izO6ED01OBnSBXcBX4EUewxDFsWC-YhCQ4-85U9\"\n",
    "HEAD_API  = {\"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "HEAD_HTML = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123 Safari/537.36\"}\n",
    "\n",
    "SEARCH_URL = \"https://api.genius.com/search\"\n",
    "SONG_URL   = \"https://api.genius.com/songs/{id}\"\n",
    "\n",
    "CHECKPOINT_FILE = \"unique_songs_with_meta_progress.json\"\n",
    "SAVE_EVERY = 25\n",
    "SLEEP_SEC  = 0.25\n",
    "TEST_LIMIT = None\n",
    "\n",
    "required_cols = [\"title\", \"artist\"]\n",
    "for rc in required_cols:\n",
    "    if rc not in unique_songs_df.columns:\n",
    "        raise ValueError(f\"Missing column in unique_songs_df: {rc}\")\n",
    "\n",
    "# Create the needed new columns for the data\n",
    "for c in [\"genius_id\",\"language\",\"primary_tag\",\"tags\",\"genius_url\",\"release_date\",\"album\",\"lyrics\"]:\n",
    "    if c not in unique_songs_df.columns:\n",
    "        unique_songs_df[c] = pd.NA\n",
    "\n",
    "\n",
    "### SAFETY SAVING\n",
    "# Continue the download from the check point (if there is already one)\n",
    "if Path(CHECKPOINT_FILE).exists():\n",
    "    try:\n",
    "        _ckpt = pd.read_json(CHECKPOINT_FILE)\n",
    "        cols = [\"title\",\"artist\",\"genius_id\",\"language\",\"primary_tag\",\"tags\",\"genius_url\",\"release_date\",\"album\",\"lyrics\"]\n",
    "        _ckpt = _ckpt[[c for c in cols if c in _ckpt.columns]].drop_duplicates(subset=[\"title\",\"artist\"])\n",
    "        unique_songs_df = unique_songs_df.merge(_ckpt, on=[\"title\",\"artist\"], how=\"left\", suffixes=(\"\", \"_ckpt\"))\n",
    "\n",
    "        for c in [\"genius_id\",\"language\",\"primary_tag\",\"tags\",\"genius_url\",\"release_date\",\"album\",\"lyrics\"]:\n",
    "            ck = f\"{c}_ckpt\"\n",
    "            if ck in unique_songs_df.columns:\n",
    "                mask = unique_songs_df[c].isna() & unique_songs_df[ck].notna()\n",
    "                unique_songs_df.loc[mask, c] = unique_songs_df.loc[mask, ck]\n",
    "                unique_songs_df.drop(columns=[ck], inplace=True, errors=\"ignore\")\n",
    "        print(f\"Continue from the checkpoint: {_ckpt.shape[0]} rows are read.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Checkpoint reading error: {e}. Starting from the beginning.\")\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "# Search for the songs\n",
    "def genius_search_with_hit(title: str, artist: str):\n",
    "    q = f\"{title} {artist}\"\n",
    "    try:\n",
    "        r = requests.get(SEARCH_URL, headers=HEAD_API, params={\"q\": q}, timeout=20)\n",
    "        if not r.ok:\n",
    "            return None, None\n",
    "        hits = (r.json().get(\"response\", {}) or {}).get(\"hits\", []) or []\n",
    "        if not hits:\n",
    "            return None, None\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "    t0 = unidecode(str(title)).strip().lower()\n",
    "    a0 = unidecode(str(artist)).strip().lower()\n",
    "\n",
    "    for h in hits:\n",
    "        res = h.get(\"result\", {}) or {}\n",
    "        st = unidecode(res.get(\"title\",\"\")).strip().lower()\n",
    "        sa = unidecode((res.get(\"primary_artist\") or {}).get(\"name\",\"\")).strip().lower()\n",
    "        if st == t0 and sa == a0:\n",
    "            return res.get(\"id\"), res\n",
    "\n",
    "    res = (hits[0].get(\"result\", {}) or {})\n",
    "    return res.get(\"id\"), res\n",
    "\n",
    "\n",
    "# Search for the songs' meta data\n",
    "def genius_song_meta(song_id: int):\n",
    "    try:\n",
    "        r = requests.get(SONG_URL.format(id=song_id), headers=HEAD_API, params={\"text_format\":\"plain\"}, timeout=20)\n",
    "        if not r.ok:\n",
    "            return None, None, None, None, None, None\n",
    "        d = (r.json().get(\"response\", {}) or {}).get(\"song\", {}) or {}\n",
    "    except Exception:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    lang = d.get(\"language\")\n",
    "    ptag = (d.get(\"primary_tag\") or {}).get(\"name\")\n",
    "    tags = [t.get(\"name\") for t in (d.get(\"tags\") or []) if isinstance(t, dict) and t.get(\"name\")] or None\n",
    "    url  = d.get(\"url\")\n",
    "    release_date = d.get(\"release_date\")\n",
    "    album_name   = (d.get(\"album\") or {}).get(\"name\")\n",
    "\n",
    "    return lang, ptag, tags, url, release_date, album_name\n",
    "\n",
    "\n",
    "# Scrape songs' tags\n",
    "def scrape_tags_from_html(url: str):\n",
    "    try:\n",
    "        html = requests.get(url, headers=HEAD_HTML, timeout=20).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        tags = []\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            if \"/tags/\" in href:\n",
    "                txt = a.get_text(strip=True)\n",
    "                if txt and txt.lower() not in (\"about\", \"lyrics\"):\n",
    "                    tags.append(txt)\n",
    "        return list(dict.fromkeys(tags)) or None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Scrape songs' lyrics\n",
    "def scrape_lyrics_from_html(url: str):\n",
    "    try:\n",
    "        html = requests.get(url, headers=HEAD_HTML, timeout=25).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        old = soup.find(\"div\", class_=\"lyrics\")\n",
    "        if old:\n",
    "            text = old.get_text(separator=\"\\n\").strip()\n",
    "            return text if text else None\n",
    "\n",
    "        containers = soup.find_all(\"div\", class_=lambda c: c and \"Lyrics__Container\" in c)\n",
    "        if containers:\n",
    "            parts = []\n",
    "            for c in containers:\n",
    "                txt = c.get_text(separator=\"\\n\", strip=True)\n",
    "                if txt:\n",
    "                    parts.append(txt)\n",
    "            text = \"\\n\".join(parts).strip()\n",
    "            return text if text else None\n",
    "\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "### Save safety save\n",
    "def save_checkpoint(df_to_save: pd.DataFrame):\n",
    "    cols = [\"title\",\"artist\",\"genius_id\",\"language\",\"primary_tag\",\"tags\",\"genius_url\",\"release_date\",\"album\",\"lyrics\"]\n",
    "    cols = [c for c in cols if c in df_to_save.columns]\n",
    "    df_to_save[cols].to_json(CHECKPOINT_FILE, orient=\"records\", force_ascii=False)\n",
    "    print(f\"Checkpoint is saved: {CHECKPOINT_FILE}\")\n",
    "\n",
    "\n",
    "# Sample size settings -> TEST_LIMIT\n",
    "# Initially for testing the code on smaller sample size\n",
    "need_mask = unique_songs_df[[\"primary_tag\",\"tags\"]].isna().any(axis=1)\n",
    "need_idx = unique_songs_df[need_mask].index.tolist()\n",
    "if TEST_LIMIT:\n",
    "    need_idx = need_idx[:TEST_LIMIT]\n",
    "\n",
    "print(f\"Running on – {len(need_idx)} rows (genre/tags + release_date + album + lyrics).\")\n",
    "\n",
    "\n",
    "# Main cycle\n",
    "for n, idx in enumerate(tqdm(need_idx, desc=\"Backfill…\"), start=1):\n",
    "    title  = unique_songs_df.at[idx, \"title\"]\n",
    "    artist = unique_songs_df.at[idx, \"artist\"]\n",
    "\n",
    "    # 1) Genius ID\n",
    "    sid = unique_songs_df.at[idx, \"genius_id\"]\n",
    "    hit_res = None\n",
    "    if pd.isna(sid) or sid in (None, \"\", 0):\n",
    "        sid, hit_res = genius_search_with_hit(title, artist)\n",
    "        unique_songs_df.at[idx, \"genius_id\"] = sid\n",
    "    else:\n",
    "        try:\n",
    "            sid = int(sid)\n",
    "        except Exception:\n",
    "            sid, hit_res = genius_search_with_hit(title, artist)\n",
    "            unique_songs_df.at[idx, \"genius_id\"] = sid\n",
    "\n",
    "    if not sid:\n",
    "        if n % SAVE_EVERY == 0:\n",
    "            save_checkpoint(unique_songs_df)\n",
    "        time.sleep(SLEEP_SEC)\n",
    "        continue\n",
    "\n",
    "    # 2) /songs/:id meta\n",
    "    lang = ptag = tags = url = rel = alb = None\n",
    "    lang, ptag, tags, url, rel, alb = genius_song_meta(int(sid))\n",
    "\n",
    "    if pd.isna(unique_songs_df.at[idx, \"language\"]) and lang is not None:\n",
    "        unique_songs_df.at[idx, \"language\"] = lang\n",
    "    if url and pd.isna(unique_songs_df.at[idx, \"genius_url\"]):\n",
    "        unique_songs_df.at[idx, \"genius_url\"] = url\n",
    "    if pd.isna(unique_songs_df.at[idx, \"release_date\"]) and rel is not None:\n",
    "        unique_songs_df.at[idx, \"release_date\"] = rel\n",
    "    if pd.isna(unique_songs_df.at[idx, \"album\"]) and alb is not None:\n",
    "        unique_songs_df.at[idx, \"album\"] = alb\n",
    "\n",
    "    # 3) search-hit fallback for the genre\n",
    "    if (pd.isna(unique_songs_df.at[idx,\"primary_tag\"]) or unique_songs_df.at[idx,\"primary_tag\"] is None) and hit_res:\n",
    "        pr = hit_res.get(\"primary_tag\") or {}\n",
    "        pr_name = pr.get(\"name\")\n",
    "        if pr_name:\n",
    "            unique_songs_df.at[idx,\"primary_tag\"] = pr_name\n",
    "\n",
    "        hit_tags = hit_res.get(\"tags\")\n",
    "        if (pd.isna(unique_songs_df.at[idx,\"tags\"]) or unique_songs_df.at[idx,\"tags\"] is None) and hit_tags and isinstance(hit_tags, list):\n",
    "            unique_songs_df.at[idx,\"tags\"] = hit_tags\n",
    "\n",
    "        if pd.isna(unique_songs_df.at[idx,\"album\"]):\n",
    "            alb_hit = (hit_res.get(\"album\") or {}).get(\"name\") if isinstance(hit_res.get(\"album\"), dict) else None\n",
    "            if alb_hit:\n",
    "                unique_songs_df.at[idx,\"album\"] = alb_hit\n",
    "\n",
    "    # 4) HTML fallback for the genre (tags/primary_tag)\n",
    "    use_url = url if url else (unique_songs_df.at[idx,\"genius_url\"] if pd.notna(unique_songs_df.at[idx,\"genius_url\"]) else None)\n",
    "    if (pd.isna(unique_songs_df.at[idx,\"tags\"]) or unique_songs_df.at[idx,\"tags\"] is None) and use_url:\n",
    "        html_tags = scrape_tags_from_html(use_url)\n",
    "        if html_tags:\n",
    "            unique_songs_df.at[idx,\"tags\"] = html_tags\n",
    "        if (pd.isna(unique_songs_df.at[idx,\"primary_tag\"]) or unique_songs_df.at[idx,\"primary_tag\"] is None) and html_tags:\n",
    "            unique_songs_df.at[idx,\"primary_tag\"] = html_tags[0]\n",
    "\n",
    "    # 5) lyrics\n",
    "    if (pd.isna(unique_songs_df.at[idx,\"lyrics\"]) or not isinstance(unique_songs_df.at[idx,\"lyrics\"], str) or not unique_songs_df.at[idx,\"lyrics\"]) and use_url:\n",
    "        song_lyrics = scrape_lyrics_from_html(use_url)\n",
    "        if song_lyrics:\n",
    "            unique_songs_df.at[idx,\"lyrics\"] = song_lyrics\n",
    "\n",
    "    # 6) Timer and checkpoint\n",
    "    if n % SAVE_EVERY == 0:\n",
    "        save_checkpoint(unique_songs_df)\n",
    "    time.sleep(SLEEP_SEC)\n",
    "\n",
    "# Final save\n",
    "save_checkpoint(unique_songs_df)\n",
    "print(\"Done: checkpoint is updated.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
